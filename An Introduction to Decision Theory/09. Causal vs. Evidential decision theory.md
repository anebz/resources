# 9. Causal vs. Evidential decision theory

The focus of this chapter is on the role of causal processes in decision making. In some decision problems, beliefs about causal processes play a significant role in determining what we intuitively think it's rational to do, but it's very difficult to give a convincing account of what role beliefs about causal processes should be allowed to play.

## 9.1. Newcomb's problem

There's a being who predicts other people's choices very well. There are 2 boxes, one with $1k inside for sure, and the being decides what to put in box2. You can either choose box1 and box2, or just box2. The being will put $1M in box2 iff she's sure that you will only pick box2. What's your decision?

* Alternative 1: box1 ($1k) and box2 (0 or $1M)
* Alternative 2: box2 (0 or $1M)

(ane: I'd go for box2)

There are two different, but contradictory ways of reasoning about the problem. It's rational to take both boxes, because you get $1k + $1M\*p, with some probability you don't know. You can't influence the being, so whatever decision she makes, by the time it's your turn, the money is either in box2 or not, no matter what you do. So you take both, because you will better off, come what may. This is an application of the dominance principle: two boxes dominates taking just one.

But if you take both boxes you haven't taken all relevant information into account. It's also rational to consider that the predictor has predicted your choice and adjusted the amounts of money in box2. The probability that the predictor will make a correct choice is 99%. So if you're going to take both boxes, the predictor will predict that. And if you take just box2, the predictor will predict that as well. So you take box2. This is the principle of maximizing expected utility.

We assume that your utility of money is linear, so u($1M) = 1M

| | Second box contains $1m  | Second box empty | 
| - | - | - |
| Take box2 only | $1M (p=0.99) | 0 (p=0.01) |
| Take both boxes | $1.001M (p=0.01) | $1k (p=0.99) |

The expected utility of taking box2 is much bigger, so we have two contradictory principles. Which do we choose?

## 9.2. Causal decision theory

It's the view that a rational dm should keep all her beliefs about causal processes fixed in the decision kaing process, and always choose an alternative that is optimal according to these beliefs.

Causal decision theory recommends you to take the two boxes. Since you know that the predictor has already placed or not placed $1M in box2, nothing you make now could somehow affect the probability of finding $1M in box2.

X [] > Y abbreviates the proposition "IF the dm were to do X, then Y would be the case".

## 9.3. Evidential decision theory

Some decision theorists claim there are cases in which causal decision theory yields counterintuitive recommendations.

> Paul is told that the number of psychopaths in the world is fairly low. Paul is debating whether to press the "kill all psychopaths" button. He thinks it'd be better to live in a world with no psychopaths. But Paul is confident only a psychopath would press that button, and Paul prefers living in a world with some psychopaths than dying. Does Paul press the button?

(ane: don't press it)

Causal decision theory incorrectly implies that Paul should press the button. p(press [] > dead) is much lower than p(press [] > world without psychopaths). This is because Paul is either a psychopath or not, and the probability of the two possibilities doesn't depend on what he does (or does it?).

Let's assume that p(press [] > dead) = 0.001, and p(press [] > world without psychopaths) = 0.999. The utility of death is -100, and the utility of living in a world without psychopaths is +1.

* Press button: p(press [] > dead) * u(dead) + p(press [] > world without psychopaths) * u(world without psychopaths) = 0.899
* Don't press: 0

According to the causal decision theory, Paul should press. But evidential decision theorists explicitly deny the causal analysis. They think that causal theorists calculate probabilities in the wrong way. It's almost certain that if you press, you are a psychopath, and this should be accounted for somehow. Evidential decision theorists agree that pressing the button doesn't cause any psychiatric disease, but if you pressed the button you'd learn something about yourself you didn't know, maybe that you're a psychopath.

In their opinion, probabilities should not be p(X [] > Y), but rather p(X [] > Y | X), and this changes things.

An objection to evidential decision theory is that it requires that the dm can somehow ascribe probabilities to her own choices. And this is incoherent because one's own choices are not the kind of things one can reasonably ascribe probabilities to.

If probabilities are taken to be objective, it seems difficult to reconcile the idea of a dm making a free choice with the thought that your choices are somehow governed by probabilistic processes. How can something be the outcome of a deliberative process *and* a random event?

If the dm assigns subjective probabilities to his own choices, this readiness to accept bets becomes an instrument for measuring his own underlying dispositions to act. If the dm ascribes subjective probabilities to his present alternatives, then he must be prepared to take on bets on which act he will eventually choose. By taking on such bets, it becomes more attractive fo the dm to perform the act he√üs betting on. And the 'measurement instrument' (the bets used for eliciting subjective probabilities) will interfere with the entity being measured.

