# 4. Decisions under risk

Unlike in decisions under ignorance, the dm knows the probabilities of the outcomes. In this chapter we will focus whether it's plausible to think that the principle of maximizing expected value is the appropriate decision rule to apply to decisions under risk.

Nearly all decision theorists agree that this is the best method, wheras regarding decisions under ignorance there is absolutely no consensus.

## 4.1. Maximizing what?

The principle of maximizing expected monetary value isn't the same as the principle of maximizing expected value. There's also the principle of maximizing expected utility, which is more precise than the expected value principle.

Example: you're offered $1M or a lottery ticket that gives you 50% chance of winning $3M, or nothing. The expected monetary value (EMV) is calculated as such, where `p` is the probability of the state and `m` the monetary value

```math
EMV = \sum_{i=1}^n pi*mi
```

ENV(lottery A) = $1M, ENV(lottery b) = $1.5M. But many people would choose A, because the distance between $1M and $3M in our minds is much smaller than the distance between 0 and $1M. Winning more is always better than winning less, but the more one wins, the lower the value of winning yet another million. The *marginal value* of money decreases. 

But this marginal value might change depending on how much money you have, if you're a multibillionaire, maybe having one more million increases your chances of being kidnapped. Instead, we define the expected value (EV), where `v` is the value rather than the money:

```math
EV = \sum_{i=1}^n pi*vi
```

But not all values are reliable guides to rational decision making, like moral value for instance. Someone donating money might benefit the recipients but not the giver. To single out the kind of value that's the primary object of study in decision theory - the value of an outcome as evaluated from the dm's point of view - we introduce the concept of **utility**.

> The utility of an outcome depends on how valuable the outcome is from the dm's point of view.

```math
EU = \sum_{i=1}^n pi*ui
```

## 4.2. Why is it rational to maximize EU?

Decision theorists propose two fundamentally different arguments:

1. Law of large numbers: in the long run, you'll be better off if you maximize EU
2. It aims to derive the EU principle from some more fundamental axioms for rational decision making

The LLN states that everyone maximizing EU will almost certainly be better off in the long run. If you repeat an experiment sufficiently many times, the probability that the average outcome differs from the expected outcome can be rendered arbitrarily small.

The relevance of LLN is partially defeated by the *gambler's ruin* mathematical theorem: if we're tossing a coin and each time someone wins, they get $1, and we both start with X$, at some point someone will go bankrupt and won't be able to pay. LLN only works only if the initial pot, or amount of money in this example, is infinite. This is a rather unrealistic assumption.

Another objection to LLN is that it's difficult in real life for dms to face the exact same decision problem several times. In the roulette, the dealer might do it slightly different every time.

Another objection to LLN is that many decisions under risk are unique in a much stronger sense, people marry each other (or don't) once, and don't think about LLN when facing that decision.

## 4.3. The axiomatic approach

This approach seeks to show that the EU principle can be derived from axioms that hold independently of what happens in the long run. There are two fundamentally different strategies to axiomatize the EU principle:

### Indirect approach (dominant approach)

The dm doesn't prefer a risky act to another because the EU of the former exceeds that of the latter. The dm is asked to state a set of preferences over a set of risky acts. It's irrelevant how these preferences are generated. If the set of preferences stated by the dm is consistent with some axioms, it can be shown that her decisions can be described as if she were choosing what to do by assigning numerical probabilities and utilities to outcomes and then maximizing EU. More on this in the chapters 5, 7, 8.

### Direct approach

It seeks to generate preferences over acts from probabilities and utilities directly assigned to outcomes. It's not assumed that the dm has access to a set of preferences over acts before she starts to deliberate. The aim of the axiomatization is to show that the utility of an act equals the expected utility of its outcomes. 

The first axiom holds that if all outcomes of an act have utility `u`, then the utility of the act is `u`.

The second axiom is the dominance principle: if one act is certain to lead to outcomes with higher utilities under all states, then the utility of the former act exceeds that of the latter. This axiom requires states to be causally independent of acts.

The third axiom holds that every decision problem can be transformed into a decision problem with equiprobable states by splitting the original states into parallel ones without affecting the overall utility of any of the acts in the decision problem.

The fourth axiom is a trade-off principle. If 2 outcomes are equally probable, and if the best outcome is made slightly worse, then this can be compensated for by adding *some* amount of utility to the other outcome. And then I might change my opinion if I want to take the risk or not.

## 4.4. The Allais paradox

| | Ticket nº 1  | Tickets n1 2-11 | Tickets nº 12-100 |
| - | - | - | - |
| Gamble 1 | $1M  | $1M | $1M |
| Gamble 2 | 0  | $5M | $1M |
| Gamble 3 | $1M  | $1M | 0 |
| Gamble 4 | 0  | $5M | 0 |

In a choice between G1 and G2, most people would choose G1. And in a choice between G3 and G4, most people would choose G4. They'd believe it's rational to trade a ten-in-hundret chance of getting $5M against a one-in-hundred risk of getting nothing. But if we calculate the differences:

```math
EU(G1) - EU(G2) = u(1M) - (0.01u(0M) + 0.1u(5M) + 0.89u(1M)) = 0.11u(1M) - 0.01u(0M) - 0.1u(5M)
EU(G3) - EU(G4) = (0.11u(1M) + 0.89u(0M)) - (0.9u(0M) + 0.1u(5M)) = 0.11u(1M) - 0.01u(0M) - 0.1u(5M)
```

The difference is the exact same thing! It's impossible to prefer G1 to G2 and G3 to G4 without violating the EU principle. But still, people, after thinking hard about it still say it's rational to do it, so there must be something wrong with the EU principle.

If a ticket between 12-100 is drawn, it doesn't matter which one to choose between G1-G2 and G3-G4. But in the 1-11 cases, G1 and G2 are parallel, and G3 and G4 are parallel. Savage makes a point that dms should base their decisions entirely on features that differ between alternatives: *sure-thing principle*, discussed in more detail in Chapter 7.

Also, getting $0M in G2 is different than getting $0M in G4. In G2, we get $0 and disappointment.

## 4.5. The Ellsberg paradox

| | 30  | 60 | 60 |
| - | - | - | - |
| | Red | Black | Yellow |
| Gamble 1 | $100  | 0 | 0 |
| Gamble 2 | 0  | $100 | 0 |
| Gamble 3 | $100  | 0 | $100 |
| Gamble 4 | 0  | $100 | $100 |

Between G1,G2, most people (me included) choose G1, because we know how many red balls there are. Between G3,G4, most people (me included) choose G4, because the probability is known. We get 60/90 probability of getting $100.

The point of this example is: no matter what the dm's utility for money is, and no matter what she believes about the proportion of black and yellow balls in the urn, the principle of maximizing EU can never recommend G1 > G2 and G4 > G3. That's because EU(G1) > Eu(G2) iff EU(G3) > Eu(G4). The difference in EUs is the same, as in the other paradox.

The Ellsberg paradox arises because we want to avoid epistemic uncertainty about probabilities, and Allais paradox arises because we want to avoid uncertainty about outcomes.

## 4.6. The St. Petersburg paradox

Discovered by Bernoulli, this is based in the St. Petersburg game, where you toss a coin until it lands heads up. Then, you get 2<sup>n</sup> of utility. How much utility should you be willing to 'pay' for the opportunity to play this game? According to EU principle, infinite. But this is absurd, because the most likely outcome is that you win a rather small amount of utility. Should you be willing to pay a billion units of utility for playing a game in which you get no more than X units back with such high probability?

Another theorist argues that sufficiently improbable outcomes shold be regarded as morally impossible. i.e. beyond concern. A rational dm should simply disregard the possibility of winning a very high amount, since such an outcome is highly improbable. This idea is closely related to the principle of minimal risks, which holds that sufficiently improbable outcomes should be ignored.

Other theorists have tried to resolve the paradox by setting an upper limit on the dm's utility scale.

## 4.8. The two-envelope paradox

There are two envelopes with some money in it, and a trustworthy informant tells you one has double the money than the other. You pick envelope A at random. Before you can open it, you're offered to swap. The following reasoning says you ought to swap.

Let's assume there's x money in A. Then B has either 2x or x/2 money. Both possibilities are equally likely, so the expected monetary value of swapping is: 0.5 \* 2x + 0.5 \* x/2 = 5x/4. Which is bigger than x, so you swap.

When you're about to open B, you're offered to swap again. The following reasoning says you ought to swap: (same as before but with mirrored).

There must be something wrong with this, it's not possible that the EMV(A) > EMV(B) and EMV(B) > EMV(A) at the same time.

| | 1/2  | 1/2 | Expected value |
| - | - | - | - |
| Envelope A | x | x | x |
| Envelope B | 2x | x/2 | 5x/4 |

| | 1/2  | 1/2 | Expected value |
| - | - | - | - |
| Envelope A | 2y | y/2 | 5y/4 |
| Envelope B | y | y | y |

The present formulation of the paradox presupposes that there's no upper limit to how much money there is in the world. Imagine there's an upper limit L for the money in the world. Then, no envelope can contain more than 2L/3, in which case the other envelope would contain L/3. If A has 2L/3, then it's not true that B can have either 4L/3 or L/3. 
