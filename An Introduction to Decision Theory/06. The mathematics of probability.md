# 6. The mathematics of probability

Monty Hall problem:

> There are three doors, and behind them a car and 2 goats. Monty Hall knows what's behind them. You pick a door without opening it. Then he opens another door, always containing a goat. Then he gives you the opportunity to switch doors or keep the one you have. Should you accept Monty Hall's offer?

All decision theorists say you ought to switch, because it's irrational not to. Monty Hall's behavior reveals some extra information that makes it easier for you to locate the car.

* If you pick a door with a goat, then Monty Hall has no option to open the other 2 doors, he can only open the one with the other goat. So he indirectly reveals where the car is: then you should definitely switch.
* If you pick a door with a car, you should not switch.

In the beginning, you have 2/3 probability to have picked a goat. And because it's certain that if you first get a goat and then switch, you have 2/3 probability of winning if you follow this strategy. Doing this, you win 2 out of 3 times.

## 6.1. The probability calculus

Axioms proposed by Kolmogorov in 1933:

1. Every probability is a real number between 0 and 1: 1 >= p(A) >= 0
2. The probability of the entire sample space is 1: p(S) = 1
3. If two events are mutually exclusive, then the probability that one of them will occur equals the probability of the first + the probability of the second: If A x B = 0. then p(A + B) = p(A) + p(B)

(x is used as intersection and + as union when dealing with sets)

The philosophical approach of writing these axioms:

1. 1 >= p(A) >= 0
2. If A is a logical truth, then p(A) = 1
3. If A and B are mutually exclusive, then p(A + B) = p(A) + p(B)

The mathematical way assigns probabilities to events, the things that take place in the external world. The second assigns probabilities to linguistic entities, propositions. The kind of things we use when we reason and make inferences.

## 6.2. Conditional probability

Some definitions:

* p(A|B) = p(AB)/p(B)
* A is independent of B iff p(A) = p(A|B)
* if A is independent of B, then p(AB) = p(A) * p(B)

## 6.3. Bayers' theorem

p(B|A) = p(A|B) * p(B) / p(A), given that p(A) > 0