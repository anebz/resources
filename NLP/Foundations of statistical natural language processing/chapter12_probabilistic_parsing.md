# Chapter 12: Probabilistic parsing

Parsing can be considered as chinking, recognizing higher level units of structure that allows to compress the description of a sentence. One way to capture the regularity of chunks over different sentences is to learn a grammar that explains the structure of the chunks one finds --> grammar induction.

Before parsing, we need to know what we'll do with them: we can use syntactic structure as a first step towards semantic interpretation, detecting phrasal chunks for indexing in an IR system, or trying to build a probabilistic parse that outperforms n-gram models. For any of these tasks, the goal is to produce a system that can place a provably useful structure over abritrary sentences, that is, to build a parser. for this goal, one begins with a tabula rasa. if we want to produce useful syntactic structure, we should use all the prior information we have. 

## 12.1. Some concepts

### 12.1.1. Parsing for disambiguation

Three ways to use probabilities in a parser:

1. Probabilities for determining the sentence: use a parser as a language model over a word lattice to determine what sequence of words running along a path through the lattice has the highest probability.
2. Probabilities for speedier parsing: order or prune the search space of a parser, enable the parser to find the best parse faster while not harming the quality of the results being produced.
3. Probabilities for choosing between parses: choose from among the many parses of the input sentence, which ones are most likely. 

Capturing the tree structure of a particular sentence is the key to the goal of disambiguation. (examples about trees in 410-411)

### 12.1.2. Treebanks

Grammar induction approaches usually don't produce the parse trees that people want, which are what are called treebanks (Penn treebank, p413.

Many people argue that it's better to have linguists constructing treebanks than grammars, because it's easier to work out the correct parse of individual actual sentences than to try to determine (often by intuition) what all possible manifestations of a certain rule are. 

### 12.1.3. Parsing models vs. language models

The idea of parsing is to be able to take a sentence 's' and to work out parse trees for it according to some grammar G. We want to ranke possible parses showing how likely each one is, or maybe to return the most likely parse of a sentence --> define a probabilistic parsing model, which evaluates the probability of trees 't' for a sentence 's' by finding P(t|s, G). Given a probabilistic parsing model, the job of a parser is to find the most probable parse of a sentence 'x'. 

One can directly estimate a parsing model, and people have done this, but in general we need to base our probability estimates on some more general class of data --> define a language model, which assigns a probability to all trees generated by the grammar, then examine P(t,s|G). 

A language model can always be used as a parsing model if we want to choose between parses, but a language model can also be used for other purposes. but there's no direct way to convert an arbitrary parsing model into a language model.

More on PCFGs, priming, lexicalization, tree probabilities, applications, dependency grammar, evaluation, equivalent models, etc in p416-438. 

Tableau, Viterbi algorithm, A* search, other methods in p441-450
